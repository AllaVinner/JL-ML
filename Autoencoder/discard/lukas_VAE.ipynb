{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Test implementation of a Variational Autoencoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from keras import layers, losses\r\n",
    "\r\n",
    "from keras.datasets import mnist\r\n",
    "from keras.models import Model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(x_train, _), (x_test, y_test) = mnist.load_data()\r\n",
    "\r\n",
    "print(x_train.shape)\r\n",
    "\r\n",
    "def preprocess_images(images, discrete=True):\r\n",
    "    # Normalize and make into tensor\r\n",
    "    images = images.reshape((images.shape[0], 28, 28, 1)) /255.\r\n",
    "\r\n",
    "    if discrete:\r\n",
    "        # Round pixel values to 0 or 1. Allows use of cross entropy loss?\r\n",
    "        return np.where(images > .5, 1.0, 0.0).astype('float32')\r\n",
    "    else:\r\n",
    "        # No discretization of pixel values\r\n",
    "        return images.astype('float32')\r\n",
    "\r\n",
    "# Look at random image (before binarization)\r\n",
    "img_nbr = np.random.randint(0, len(x_train)+1)\r\n",
    "plt.imshow(x_train[img_nbr, :, :])\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# Don't discretize if continuous bernoulli loss is being used\r\n",
    "discrete=True\r\n",
    "\r\n",
    "x_train = preprocess_images(x_train, discrete=discrete) # shape: (nbr_images, 28, 28, 1)\r\n",
    "x_test = preprocess_images(x_test, discrete=discrete)\r\n",
    "\r\n",
    "print(x_train.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Do we need to batch and shuffle the data?\r\n",
    "# Maybe in we dont use the fit() function, we need to do it manually\r\n",
    "\r\n",
    "train_size = x_train.shape[0]\r\n",
    "batch_size = 32 # Size of batches in gradient update / training step?\r\n",
    "test_size = x_test.shape[0]\r\n",
    "print(x_test.shape)\r\n",
    "\r\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(x_train).shuffle(train_size).batch(batch_size))\r\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(x_test).shuffle(test_size).batch(batch_size))\r\n",
    "\r\n",
    "tf.print(test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternative functional approach\n",
    "\n",
    "https://keras.io/examples/generative/vae/\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define model\r\n",
    "# https://www.tensorflow.org/tutorials/generative/cvae\r\n",
    "\r\n",
    "class VAE(Model):\r\n",
    "    def __init__(self, latent_dim=2, dropout_rate=0.1):\r\n",
    "        super(VAE, self).__init__()\r\n",
    "        self.latent_dim = latent_dim\r\n",
    "\r\n",
    "        self.encoder = keras.Sequential([\r\n",
    "            layers.InputLayer(input_shape=(28, 28, 1)),\r\n",
    "\r\n",
    "            layers.Conv2D(\r\n",
    "                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\r\n",
    "            layers.Dropout(dropout_rate),\r\n",
    "            \r\n",
    "            layers.Conv2D(\r\n",
    "                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\r\n",
    "            layers.Dropout(dropout_rate),\r\n",
    "            \r\n",
    "            layers.Flatten(),\r\n",
    "\r\n",
    "            # 2D latent space -> 2D mean, 2D var ->\r\n",
    "            # mean1, mean2, log_var1, log_var2 -> 2 + 2\r\n",
    "            layers.Dense(latent_dim + latent_dim),\r\n",
    "        ])\r\n",
    "\r\n",
    "        self.decoder = keras.Sequential([\r\n",
    "            # Point z sampled from distrs. will be 2D\r\n",
    "            layers.InputLayer(input_shape=(latent_dim,)),\r\n",
    "\r\n",
    "            # BIG dense that takes 2d z input\r\n",
    "            layers.Dense(units=7*7*32, activation='relu'),\r\n",
    "\r\n",
    "            # Reshape into tensor\r\n",
    "            layers.Reshape(target_shape=(7, 7, 32)),\r\n",
    "\r\n",
    "            # Transp. Conv: padding same? no upsampling?\r\n",
    "            # Pads the input so output is same dim?\r\n",
    "            layers.Conv2DTranspose(\r\n",
    "                filters=64, kernel_size=3, strides=(2, 2), padding='same',\r\n",
    "                activation='relu'),\r\n",
    "            layers.Dropout(dropout_rate),\r\n",
    "            \r\n",
    "            layers.Conv2DTranspose(\r\n",
    "                filters=32, kernel_size=3, strides=(2, 2), padding='same',\r\n",
    "                activation='relu'),\r\n",
    "            layers.Dropout(dropout_rate),\r\n",
    "\r\n",
    "            # Last layer, no activation / linear?\r\n",
    "            layers.Conv2DTranspose(\r\n",
    "                filters=1, kernel_size=3, strides=(1, 1), padding='same'),\r\n",
    "        ])\r\n",
    "\r\n",
    "    # Decorator: turns py code into graph\r\n",
    "    # Use for computationally expensive functions\r\n",
    "    @tf.function\r\n",
    "    def sample(self, eps=None):\r\n",
    "        # Sample 100 random z?\r\n",
    "        if eps is None:\r\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\r\n",
    "        return self.decode(eps, apply_sigmoid=True)\r\n",
    "    \r\n",
    "    def encode(self, x, training=None):\r\n",
    "        mean, logvar = tf.split(\r\n",
    "            self.encoder(x, training=training), num_or_size_splits=2, axis=1)\r\n",
    "        return mean, logvar\r\n",
    "\r\n",
    "    def reparameterize(self, mean, logvar):\r\n",
    "        # Returns a z sampled from N(mean, var)\r\n",
    "        \r\n",
    "        # Added stuff to deal with batches?\r\n",
    "        # Causes some error with @tf.function though?\r\n",
    "        batch = tf.shape(mean)[0]\r\n",
    "        dim = tf.shape(mean)[1]\r\n",
    "        \r\n",
    "        eps = tf.random.normal(shape=(batch, dim))\r\n",
    "        return mean + eps * tf.exp(logvar * .5) # mu + eps * std\r\n",
    "\r\n",
    "    def decode(self, z, apply_sigmoid=False, training=None):\r\n",
    "        # Decode latent z into image\r\n",
    "        # apply_sigmoid: outputs pixel values [0, 1]\r\n",
    "        logits = self.decoder(z, training=training) # logits vals: (-inf, inf)?\r\n",
    "        if apply_sigmoid:\r\n",
    "            probs = tf.sigmoid(logits)\r\n",
    "            return probs\r\n",
    "        return logits\r\n",
    "\r\n",
    "    # Needed for summary\r\n",
    "    def call(self, x, training=None):\r\n",
    "        mean, logvar = self.encode(x, training=training)\r\n",
    "        sampled_z = self.reparameterize(mean, logvar)\r\n",
    "        decoded_img = self.decode(sampled_z, training=training)\r\n",
    "        return decoded_img\r\n",
    "    \r\n",
    "    def summary(self):\r\n",
    "        # Uses 'call' to initialize something in TF backend \r\n",
    "        # that allows for 'summary'\r\n",
    "        x = keras.Input(shape=(28, 28, 1))\r\n",
    "        model = Model(inputs=[x], outputs=self.call(x))\r\n",
    "        return model.summary()\r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow_probability as tpb # For Bernoulli distr. (tensorflow==2.5)\r\n",
    "\r\n",
    "# Defining loss function\r\n",
    "\r\n",
    "# learning rate: 1e-4\r\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\r\n",
    "\r\n",
    "# Returns logarithm of normal pdf w. \r\n",
    "# mean, logvar for each value in 'sample'\r\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\r\n",
    "    log2pi = tf.math.log(2. * np.pi)\r\n",
    "\r\n",
    "    # log(norm_pdf(sample; mean, var))\r\n",
    "    values = -.5 * (logvar + log2pi + \r\n",
    "    (sample - mean)**2 * tf.exp(-logvar))\r\n",
    "\r\n",
    "    # 'values' consists of log pdf in both latent x- and y-directions\r\n",
    "    # 'reduce_sum' sums these up to one value\r\n",
    "    return tf.reduce_sum(values, axis=raxis)\r\n",
    "\r\n",
    "\r\n",
    "def compute_loss(model, x, loss_type='bce'):\r\n",
    "    # 'bce': Binary cross entropy (defualt)\r\n",
    "    # 'bernoulli': Bernoulli loss\r\n",
    "    # 'cb': continous bernoulli loss\r\n",
    "    \r\n",
    "    if loss_type == 'bernoulli':\r\n",
    "        return compute_bernoulli_loss(model, x)\r\n",
    "    \r\n",
    "    if loss_type == 'cb':\r\n",
    "        return continuous_bernoulli_loss(model, x)\r\n",
    "    \r\n",
    "    # Compute ELBO loss w.r.t a sample x (or batch?)\r\n",
    "    mean, logvar = model.encode(x, training=True) # Training True: enable dopout (handled by Keras?)\r\n",
    "    z = model.reparameterize(mean, logvar)\r\n",
    "    x_logit = model.decode(z, training=True)\r\n",
    "\r\n",
    "    # Loss between input & output (reconstruction). Binary cross entropy\r\n",
    "    # indep. for each pixel?\r\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(\r\n",
    "        logits=x_logit, labels=x)\r\n",
    "    \r\n",
    "    log_px_z = -tf.reduce_sum(cross_ent, axis=[1,2,3])\r\n",
    "    log_pz = log_normal_pdf(z, 0., 0.)\r\n",
    "    log_qz_x = log_normal_pdf(z, mean, logvar)\r\n",
    "\r\n",
    "    # 'mean' to approx. expectation\r\n",
    "    return -tf.reduce_mean(log_px_z + log_pz - log_qz_x)\r\n",
    "\r\n",
    "\r\n",
    "def compute_bernoulli_loss(model, x):\r\n",
    "    mean, logvar = model.encode(x, training=True)\r\n",
    "    z = model.reparameterize(mean, logvar)\r\n",
    "    x_logit = model.decode(z, training=True)\r\n",
    "    \r\n",
    "    # Likelihood over pixels is Bernoulli given by logits from decoder\r\n",
    "    log_px_z_distr = tpb.distributions.Bernoulli(logits=x_logit)\r\n",
    "    # Likelihood of input image given generated \r\n",
    "    log_px_z = tf.reduce_sum(log_px_z_distr.log_prob(x), axis=[1,2,3]) # vector of losses for each image\r\n",
    "\r\n",
    "    # for KL divergence\r\n",
    "    log_pz = log_normal_pdf(z, 0., 0.)\r\n",
    "    log_qz_x = log_normal_pdf(z, mean, logvar)\r\n",
    "    \r\n",
    "    return -tf.reduce_mean(log_px_z + log_pz - log_qz_x)\r\n",
    "\r\n",
    "# TODO: Add continous bernoulli loss\r\n",
    "\r\n",
    "@tf.function\r\n",
    "def train_step(model, x, optimizer, loss_type='bce'):\r\n",
    "    # Computes gradients and updates weights\r\n",
    "\r\n",
    "    # TODO: learn how gradient tape works\r\n",
    "    with tf.GradientTape() as tape:\r\n",
    "        loss = compute_loss(model, x, loss_type=loss_type)\r\n",
    "    \r\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\r\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Attempt at understanding & implementing continuous bernoulli loss used for MNIST\r\n",
    "# VAEs. See https://arxiv.org/abs/1907.06845\r\n",
    "\r\n",
    "def continuous_bernoulli_loss(model, x):\r\n",
    "    # 'model': Tensorflow model object\r\n",
    "    # 'x': data (images in MNIST case) shape=(nbr_examples, 28, 28 1),\r\n",
    "    # i.e output from decoder\r\n",
    "    \r\n",
    "    # Should return a tf.Tensor object w. the loss\r\n",
    "    \r\n",
    "    # Cont. Bern. helper f\r\n",
    "    def _cont_bern_log_norm(x, l_lim=0.48, u_lim=0.52):\r\n",
    "        # returns the log normalizing constant for x in (0, l_lim) U (u_lim, 1) and a Taylor approximation in\r\n",
    "        # [l_lim, u_lim].\r\n",
    "        # lower & upper limit needed for numerical stability, as the function diverges for x -> 0.\r\n",
    "        # credit: https://github.com/cunningham-lab/cb_and_cc/blob/master/cb/utils.py\r\n",
    "        \r\n",
    "        # if x is outside out limit, leave it be, else clip at lower lim\r\n",
    "        cut_x = tf.where(tf.logical_or(tf.less(x, l_lim), tf.greater(x, u_lim)), x, l_lim * tf.ones_like(x))\r\n",
    "        \r\n",
    "        # log C(lambda)\r\n",
    "        log_norm = tf.math.log(tf.math.abs(2.0 * tf.math.atanh(1 - 2.0 * cut_x))) - tf.math.log(tf.math.abs(1 - 2.0 * cut_x))\r\n",
    "        \r\n",
    "        # 4th order taylor approx around 0.5?\r\n",
    "        taylor = tf.math.log(2.0) + 4.0 / 3.0 * tf.math.pow(x - 0.5, 2) + 104.0 / 45.0 * tf.math.pow(x - 0.5, 4)\r\n",
    "        \r\n",
    "        # return log norm outside interval, taylor approx inside\r\n",
    "        return tf.where(tf.logical_or(tf.less(x, l_lim), tf.greater(x, u_lim)), log_norm, taylor)\r\n",
    "    \r\n",
    "    mean, logvar = model.encode(x, training=True)\r\n",
    "    z = model.reparameterize(mean, logvar)\r\n",
    "    x_decoded = model.decode(z, training=True, apply_sigmoid=True) # lambda(z)\r\n",
    "    x_decoded_flat = tf.reshape(x_decoded, [x_decoded.shape[0], -1, 1]) # flatten images to 1D\r\n",
    "    x_decoded_flat = tf.clip_by_value(x_decoded_flat, 1e-4, 1 - 1e-4)\r\n",
    "    \r\n",
    "    # Cont. Bernoulli normalizing constant (regularizer for sharpness?)\r\n",
    "    # log_norm_const = tf.reduce_sum(_cont_bern_log_norm(x_decoded_flat), [1, 2])\r\n",
    "    log_norm_const = _cont_bern_log_norm(x_decoded_flat)\r\n",
    "    \r\n",
    "    # Regular Bernoulli loss\r\n",
    "    # log_px_z_distr = tpb.distributions.Bernoulli(probs=x_decoded)\r\n",
    "    # log_px_z_old = tf.reduce_sum(log_px_z_distr.log_prob(x), axis=[1,2,3]) # vector of losses for each image\r\n",
    "    \r\n",
    "    x_flat = tf.reshape(x, [x.shape[0], -1, 1])\r\n",
    "    log_p_all = x_flat * tf.math.log(x_decoded_flat) + (1 - x_flat) * tf.math.log(1 - x_decoded_flat) + log_norm_const\r\n",
    "    \r\n",
    "    # For each image, sum loss over all pixels. Then calc. mean over all images in batch\r\n",
    "    log_px_z = tf.reduce_mean(tf.reduce_sum(log_p_all, axis=1))\r\n",
    "    \r\n",
    "    # for KL divergence\r\n",
    "    log_pz = log_normal_pdf(z, 0., 0.)\r\n",
    "    log_qz_x = log_normal_pdf(z, mean, logvar)\r\n",
    "    KL = tf.reduce_mean(log_pz - log_qz_x) # Single MC estimate of KL (not averaged?)\r\n",
    " \r\n",
    "    return -(log_px_z + KL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Single loss pass\r\n",
    "model = VAE(2)\r\n",
    "\r\n",
    "print(continuous_bernoulli_loss(model, x_test))\r\n",
    "\r\n",
    "# for test_batch in test_dataset.take(1):\r\n",
    "#     print(compute_loss(model, test_batch, loss='cb'))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "mean, logvar = model.encode(x_test)\r\n",
    "z = model.reparameterize(mean, logvar)\r\n",
    "x_decoded = model.decode(z, training=True, apply_sigmoid=True)\r\n",
    "\r\n",
    "import importlib\r\n",
    "import completed.cb_loss as cb\r\n",
    "importlib.reload(cb)\r\n",
    "# from cb_loss import cb_loss\r\n",
    "print(x_test.shape)\r\n",
    "\r\n",
    "print(cb.cb_vae_loss(x_test, x_decoded, mean, logvar, z))\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 28, 28, 1)\n",
      "tf.Tensor(0.15051362, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "x_test_rs = x_test.reshape(x_test.shape[0:-1])\r\n",
    "print(x_test_rs.shape)\r\n",
    "# x_decoded_rs = x_decoded.reshape(x_decoded.shape[0], 28, 28)\r\n",
    "x_decoded_rs = tf.reshape(x_decoded, x_decoded.shape[:-1])\r\n",
    "print(x_decoded_rs.shape)\r\n",
    "\r\n",
    "print(cb.cb_vae_loss(x_test_rs, x_decoded_rs, mean, logvar, z))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "tf.Tensor(0.15051362, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Model training\r\n",
    "\r\n",
    "epochs = 10\r\n",
    "latent_dim = 8\r\n",
    "nbr_examples_to_generate = 16\r\n",
    "\r\n",
    "# constant random latent vector\r\n",
    "# Used for showing samples during training?\r\n",
    "random_vector_for_generation = tf.random.normal(\r\n",
    "    shape=[nbr_examples_to_generate, latent_dim])\r\n",
    "\r\n",
    "model = VAE(latent_dim)\r\n",
    "\r\n",
    "model.summary() # Doesnt specity the layers within \"sequential\"?"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_images(model, test_dataset, nbr_examples_to_generate=16, test_sample=None):\r\n",
    "    # Generates and plots 'nbr_examples_to_generate' images.\r\n",
    "    # Specify which samples though 'test_sample',\r\n",
    "    # else random are picked from dataset\r\n",
    "    \r\n",
    "    if test_sample is not None:\r\n",
    "        mean, logvar = model.encode(test_sample)\r\n",
    "    else:\r\n",
    "        # Pick random images to generate\r\n",
    "        assert batch_size >= nbr_examples_to_generate\r\n",
    "\r\n",
    "        for test_batch in test_dataset.take(1): # 'take' picks a random batch\r\n",
    "            sample = test_batch[0:nbr_examples_to_generate, :, :, :] # pick 'nbr_ex..' samples\r\n",
    "            \r\n",
    "        mean, logvar = model.encode(sample)\r\n",
    "        \r\n",
    "    z = model.reparameterize(mean, logvar)\r\n",
    "    samples = model.sample(z)\r\n",
    "    fig = plt.figure(figsize=(6, 6))\r\n",
    "    \r\n",
    "    # Finds appropriate grid side length \r\n",
    "    # grid_len = int(np.ceil(np.log2(nbr_examples_to_generate)))\r\n",
    "    grid_len = 4\r\n",
    "    \r\n",
    "    # plot sampled images\r\n",
    "    for i in range(samples.shape[0]):\r\n",
    "        plt.subplot(grid_len, grid_len, i + 1)\r\n",
    "        plt.imshow(samples[i, :, :, 0])\r\n",
    "        plt.axis('off')\r\n",
    "    \r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_images(model, test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training\r\n",
    "import time\r\n",
    "progbar = tf.keras.utils.Progbar(len(train_dataset))\r\n",
    "\r\n",
    "loss_type = 'bce'\r\n",
    "\r\n",
    "for epoch in range(1, epochs+1):\r\n",
    "    start_time = time.time()\r\n",
    "    \r\n",
    "    for i, training_batch in enumerate(train_dataset):\r\n",
    "        train_step(model, training_batch, optimizer, loss_type=loss_type)\r\n",
    "        progbar.update(i)\r\n",
    "        \r\n",
    "    end_time = time.time()\r\n",
    "    \r\n",
    "    loss = tf.keras.metrics.Mean()\r\n",
    "    \r\n",
    "    for test_batch in test_dataset:\r\n",
    "        loss(compute_loss(model, test_batch, loss_type=loss_type))\r\n",
    "    \r\n",
    "    elbo = -loss.result()\r\n",
    "    # display.clear_output(wait=False)\r\n",
    "    print('\\n Epoch {}/{}: Test ELBO: {:.3f}, time for epoch: {:.1f}s'.format(\r\n",
    "        epoch, epochs, elbo, end_time - start_time))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_sample = next(iter(test_dataset))[:16, :, :, :]\r\n",
    "\r\n",
    "generate_images(model, test_dataset, test_sample=test_sample)\r\n",
    "\r\n",
    "# Continuous bernoulli examples!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pick a single image from out test set\r\n",
    "\r\n",
    "for test_batch in test_dataset.take(1):\r\n",
    "    sample = test_batch[0:1, :, :, :]\r\n",
    "\r\n",
    "print(sample.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Look at the difference dropout layers cause\r\n",
    "\r\n",
    "plt.figure(figsize=(10, 10))\r\n",
    "\r\n",
    "plt.subplot(1, 4, 1)\r\n",
    "plt.imshow(sample[0, :, :, 0])\r\n",
    "plt.title('input img')\r\n",
    "\r\n",
    "out = model(sample)\r\n",
    "\r\n",
    "plt.subplot(1, 4, 2)\r\n",
    "plt.imshow(out[0, :, :, 0])\r\n",
    "plt.title('decoded w/o dropout')\r\n",
    "\r\n",
    "out_train = model(sample, training=True)\r\n",
    "\r\n",
    "plt.subplot(1, 4, 3)\r\n",
    "plt.imshow(out_train[0, :, :, 0])\r\n",
    "plt.title('decoded w dropout')\r\n",
    "\r\n",
    "plt.subplot(1, 4, 4)\r\n",
    "plt.imshow(out[0, :, :, 0] - out_train[0, :, :, 0])\r\n",
    "plt.title('difference')\r\n",
    "\r\n",
    "# Difference is not zero, we're actually getting a different image when we are using dropout,\r\n",
    "# since some nodes weights are disabled"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plotta reconstruction f√∂r binary cross entropy, bernoulli,\r\n",
    "# och continuous bernoulli?\r\n",
    "\r\n",
    "n = 10\r\n",
    "\r\n",
    "img_samples = x_test[:n, :, :]\r\n",
    "img_samples_d = model(img_samples)\r\n",
    "\r\n",
    "print(img_samples.shape)\r\n",
    "print(img_samples_d.numpy().shape)\r\n",
    "\r\n",
    "img_samples[0].shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(20, 4)) # Specifies window width / height\r\n",
    "for i in range(n):\r\n",
    "    # original\r\n",
    "    ax = plt.subplot(2, n, i + 1)\r\n",
    "    plt.imshow(img_samples[i])\r\n",
    "    plt.title(\"original\")\r\n",
    "    # plt.gray()\r\n",
    "    ax.get_xaxis().set_visible(False)\r\n",
    "    ax.get_yaxis().set_visible(False)\r\n",
    "\r\n",
    "    # reconstructed\r\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\r\n",
    "    plt.imshow(img_samples_d[i])\r\n",
    "    plt.title(\"reconstructed\")\r\n",
    "    # plt.gray()\r\n",
    "    ax.get_xaxis().set_visible(False)\r\n",
    "    ax.get_yaxis().set_visible(False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd593813f0e920a894f9bede9157c42ea32ce77eaa35990e17a2847ab973ef24"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('JL-ML': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}