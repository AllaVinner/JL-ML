{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train autoencoder\n",
    "This notebook is made to train an autoencoder or a variational autoencoder on the mnist data set. It can be run locally or on golab. Checks have been implemented for colab use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is in Colab:  False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "print(\"Is in Colab: \", IN_COLAB)\n",
    "if IN_COLAB:\n",
    "    os.system('git clone https://github.com/AllaVinner/JL-ML.git')\n",
    "    os.system('pip install -e JL-ML')\n",
    "    import site\n",
    "    site.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to load\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "from jlauto.models.variational_autoencoder import VariationalAutoencoder\n",
    "from jlauto.models.autoencoder import Autoencoder\n",
    "from jlauto.models.load_premade import load_premade_model\n",
    "from jlauto.models.continuous_bernoulli_loss import continuous_bernoulli_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess mnist data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "#num_samples = 100\n",
    "train_digits = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "input_shape = train_digits.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################################\n",
    "model_type = 'autoencoder'\n",
    "model_name = 'mnist_cnn_shallow'\n",
    "optimizer  = 'adam'\n",
    "epochs     = 1\n",
    "latent_dims = [2,4,8,16]\n",
    "names       = ['ae_latent_dim_'+str(lat_dim) for lat_dim in latent_dims]\n",
    "save_path  = '../saved-models/'\n",
    "\n",
    "\n",
    "for i in range(len(names)):\n",
    "  latent_dim = latent_dims[i]\n",
    "  name = names[i]  \n",
    "  # Create and train model\n",
    "  model = load_premade_model(model_type = model_type,\n",
    "                            model_name = model_name,\n",
    "                            input_shape = input_shape,\n",
    "                            latent_dim = latent_dim)\n",
    "\n",
    "  model.compile(optimizer = optimizer,loss = 'binary_crossentropy')\n",
    "\n",
    "  model.fit(train_digits,train_digits,\n",
    "            epochs = epochs,\n",
    "            batch_size = 512)\n",
    "\n",
    "  model.save(save_path + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default configure \n",
    "config = {\n",
    "    'name': 'CHANGE',\n",
    "    'model_type': 'autoencoder',\n",
    "    'model_name': 'mnist_cnn_shallow',\n",
    "    'latent_dim': 'CHANGE',\n",
    "    'optimizer': 'adam',\n",
    "    'loss': 'binary_crossentropy', \n",
    "    'batch_size': 512,\n",
    "    'epochs': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing parameters\n",
    "changing_config = {}\n",
    "changing_config['latent_dim'] = [3,4]\n",
    "changing_config['name']   = ['ae_latent_dim_'+str(lat_dim) for lat_dim in changing_config['latent_dim']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "118/118 [==============================] - 43s 341ms/step - loss: 0.3422\n",
      "INFO:tensorflow:Assets written to: ..\\saved-models\\ae_latent_dim_3\\assets\n",
      "(28, 28, 1)\n",
      "118/118 [==============================] - 41s 326ms/step - loss: 0.3464\n",
      "INFO:tensorflow:Assets written to: ..\\saved-models\\ae_latent_dim_4\\assets\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(changing_config['name'])):\n",
    "  # update config\n",
    "  for key, values in changing_config.items():\n",
    "    config[key] = values[i]\n",
    "  \n",
    "  # Save config\n",
    "  model_path = os.path.join(save_path,config['name'])\n",
    "  os.system(f'mkdir {model_path}')\n",
    "  with open(os.path.join(save_path,config['name'],'config.yaml'), 'w') as yaml_file:\n",
    "    yaml.dump(config, yaml_file)\n",
    "\n",
    "  # Create and train model\n",
    "  model = load_premade_model(model_type = config['model_type'],\n",
    "                            model_name = config['model_name'],\n",
    "                            input_shape = input_shape,\n",
    "                            latent_dim = config['latent_dim'])\n",
    "\n",
    "  model.compile(optimizer = config['optimizer'],loss = config['loss'])\n",
    "\n",
    "  model.fit(train_digits,train_digits,\n",
    "            epochs = config['epochs'],\n",
    "            batch_size = config['batch_size'])\n",
    "\n",
    "  model.save(os.path.join(save_path, config['name']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip if in colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    os.system('zip -r ./models.zip {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill\n",
    "digit_df_list = []\n",
    "dimension_df_list = []\n",
    "#latent_df_list = []\n",
    "\n",
    "\n",
    "for name, model in model_df.iterrows():\n",
    "    print(name)\n",
    "    if model['ae_type'] == \"ae\":\n",
    "        code = model['model'].encoder(test_digits).numpy()\n",
    "        code_std = np.empty(shape = code.shape)*np.nan\n",
    "        \n",
    "    elif model['ae_type'] == \"vae\":\n",
    "        digit_distribution = model['model'].encoder(test_digits).numpy()\n",
    "        code = digit_distribution[:,0,:]\n",
    "        code_std = np.sqrt(np.exp(digit_distribution[:,1,:]))\n",
    "    \n",
    "    # Caclualte digit_df data\n",
    "    digit_distance = np.linalg.norm(code, axis = 1)\n",
    "    digit_radius = np.power(np.prod(code_std, axis = 1),1/latent_dim)\n",
    "    digit_loss = tf.reduce_mean(tf.reduce_mean(tf.keras.losses.binary_crossentropy(test_digits, model['model'](test_digits)),axis = -1),axis = -1).numpy()\n",
    "    digit_tsne = TSNE(n_components = 2).fit_transform(code)\n",
    "    \n",
    "    _digit_df = pd.DataFrame(data = {\n",
    "        'model_name' : name,\n",
    "        'label' : test_labels,\n",
    "        'loss' : digit_loss,\n",
    "        'distance' : digit_distance,\n",
    "        'radius' : digit_radius,\n",
    "        'tsne_0' : digit_tsne[:,0],\n",
    "        'tsne_1' : digit_tsne[:,1],\n",
    "    })\n",
    "    digit_df_list.append(_digit_df)   \n",
    "    \n",
    "    \n",
    "    # Calculate dimension_df data\n",
    "    value_spread = np.nanstd(code, axis = 0)\n",
    "    spread_mean = np.nanmean(code_std, axis = 0)\n",
    "    dim_index = np.arange(latent_dim)\n",
    "    \n",
    "    _dimension_df = pd.DataFrame(data = {\n",
    "        'model_name' : name,\n",
    "        'dim_index' : dim_index,\n",
    "        'value_spread' : value_spread,\n",
    "        'spread_mean' : spread_mean,\n",
    "    })\n",
    "    \n",
    "    dimension_df_list.append(_dimension_df)\n",
    "    \"\"\"\n",
    "    # Calculate latent_df data\n",
    "    latent_labels = test_labels.repeat(repeats = latent_dim)\n",
    "    latent_digit_index = np.arange(num_test).repeat(repeats = latent_dim)\n",
    "    latent_dim_index = np.tile(np.arange(latent_dim), num_test)\n",
    "    latent_value = code.reshape(-1)\n",
    "    latent_spread = code_std.reshape(-1)\n",
    "    \n",
    "    _latent_df = pd.DataFrame(data = {\n",
    "        'model_name' : name,\n",
    "        'label' : latent_labels,\n",
    "        'digit_index' : latent_digit_index,\n",
    "        'dim_index' : latent_dim_index,\n",
    "        'value' : latent_value,\n",
    "        'spread' : latent_spread\n",
    "    })\n",
    "    \n",
    "    latent_df_list.append(_latent_df)\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "digit_df = pd.concat(digit_df_list)\n",
    "dimension_df = pd.concat(dimension_df_list)\n",
    "#latent_df = pd.concat(latent_df_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "196293e968144f5642c1d9ffee4d471cc2edffa0ebaedde3b86b762e71f62b8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('jenv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
